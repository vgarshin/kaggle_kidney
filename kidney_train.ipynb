{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tifffile as tiff\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tqdm import tqdm\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet, FPN, Linknet\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "print('tensorflow version:', tf.__version__)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)\n",
    "#policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "#tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 'v61'\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 4,\n",
    "    'img_size': 256,\n",
    "    'resize': 4,\n",
    "    'batch_size': 20,\n",
    "    'epochs': 1000,\n",
    "    'patience': 30,\n",
    "    'decay': False,\n",
    "    'backbone': 'efficientnetb1', # efficientnetbX, resnet34/50, resnext50, seresnet34, seresnext50\n",
    "    'bce_weight': 1,\n",
    "    'loss': 'bce_jaccard_loss', # bce_jaccard_loss bce_dice\n",
    "    'seed': 42,\n",
    "    'split': 'strat', # 'kfold', 'group' or 'strat'\n",
    "    'mirror': False,\n",
    "    'aughard': True,\n",
    "    'umodel': 'link', # 'unet', 'fpn', 'link'\n",
    "    'pseudo': 'v55', # version num 'vXX' or 'False'\n",
    "    'lr': .0005,\n",
    "    'shift': True,\n",
    "    'external': 'ext', # 'None' or 'ext' otherwise\n",
    "    'downsample': None, # 'None' or '.25' otherwise\n",
    "    'comments': 'new data'\n",
    "}\n",
    "DATA_PATH = './data2'\n",
    "resize = PARAMS['resize']\n",
    "size = PARAMS['img_size']\n",
    "ext = PARAMS['external']\n",
    "pseudo = PARAMS['pseudo']\n",
    "if PARAMS['shift']:\n",
    "    if ext:\n",
    "        if pseudo: \n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_shft_{ext}_{pseudo}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_shft_{ext}_{pseudo}/'\n",
    "        else:\n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_shft_{ext}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_shft_{ext}/'\n",
    "    else:\n",
    "        if pseudo: \n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_shft_{pseudo}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_shft_{pseudo}/'\n",
    "        else:\n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_shft/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_shft/'\n",
    "else:\n",
    "    if ext:\n",
    "        if pseudo: \n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_{ext}_{pseudo}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_{ext}_{pseudo}/'\n",
    "        else:\n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_{ext}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_{ext}/'\n",
    "    else:\n",
    "        if pseudo: \n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}_{pseudo}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}_{pseudo}/'\n",
    "        else:\n",
    "            IMGS_PATH = f'{DATA_PATH}/tiles_r{resize}_s{size}/'\n",
    "            MSKS_PATH = f'{DATA_PATH}/masks_r{resize}_s{size}/'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "if not PARAMS['mirror']:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    STRATEGY = tf.distribute.get_strategy() \n",
    "else:\n",
    "    STRATEGY = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_masks = pd.read_csv(f'{DATA_PATH}/train.csv').set_index('id')\n",
    "df_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s) // 2):\n",
    "            start = int(s[2 * i]) - 1\n",
    "            length = int(s[2 * i + 1])\n",
    "            img[start : start + length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def show_img_n_mask(df, img_num, resize):\n",
    "    img = tiff.imread(os.path.join(f'{DATA_PATH}/train', df.index[img_num] + '.tiff'))\n",
    "    if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "    mask = enc2mask(df.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "    print(img.shape, mask.shape)\n",
    "    img = cv2.resize(img,\n",
    "                     (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                     interpolation=cv2.INTER_AREA)\n",
    "    mask = cv2.resize(mask,\n",
    "                      (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                      interpolation=cv2.INTER_NEAREST)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, alpha=.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_n_mask(df=df_masks, img_num=4, resize=PARAMS['resize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARAMS['aughard']:\n",
    "    aug = albu.Compose([\n",
    "        albu.OneOf([\n",
    "            albu.RandomBrightness(limit=.2, p=1), \n",
    "            albu.RandomContrast(limit=.2, p=1), \n",
    "            albu.RandomGamma(p=1)\n",
    "        ], p=.5),\n",
    "        albu.OneOf([\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MedianBlur(blur_limit=3, p=1)\n",
    "        ], p=.25),\n",
    "        albu.OneOf([\n",
    "            albu.GaussNoise(0.002, p=.5),\n",
    "            albu.IAAAffine(p=.5),\n",
    "        ], p=.25),\n",
    "        albu.OneOf([\n",
    "            albu.ElasticTransform(alpha=120, sigma=120 * .05, alpha_affine=120 * .03, p=.5),\n",
    "            albu.GridDistortion(p=.5),\n",
    "            albu.OpticalDistortion(distort_limit=2, shift_limit=.5, p=1)                  \n",
    "        ], p=.25),\n",
    "        albu.RandomRotate90(p=.5),\n",
    "        albu.HorizontalFlip(p=.5),\n",
    "        albu.VerticalFlip(p=.5),\n",
    "        albu.Cutout(num_holes=10, \n",
    "                    max_h_size=int(.1 * size), max_w_size=int(.1 * size), \n",
    "                    p=.25),\n",
    "        albu.ShiftScaleRotate(p=.5)\n",
    "    ])\n",
    "else:\n",
    "    aug = albu.Compose([\n",
    "        albu.OneOf([\n",
    "            albu.RandomBrightness(limit=.2, p=1), \n",
    "            albu.RandomContrast(limit=.2, p=1), \n",
    "            albu.RandomGamma(p=1)\n",
    "        ], p=.5),\n",
    "        albu.RandomRotate90(p=.25),\n",
    "        albu.HorizontalFlip(p=.25),\n",
    "        albu.VerticalFlip(p=.25)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenKid(Sequence):\n",
    "    \n",
    "    def __init__(self, imgs_path, msks_path, imgs_idxs, img_size,\n",
    "                 batch_size=32, mode='fit', shuffle=False, \n",
    "                 aug=None, resize=None):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.msks_path = msks_path\n",
    "        self.imgs_idxs = imgs_idxs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.aug = aug\n",
    "        self.resize = resize\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.imgs_idxs) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.imgs_idxs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_size = min(self.batch_size, len(self.imgs_idxs) - index*self.batch_size)\n",
    "        X = np.zeros((batch_size, self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        imgs_batch = self.imgs_idxs[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        if self.mode == 'fit':\n",
    "            y = np.zeros((batch_size, self.img_size, self.img_size), dtype=np.float32)\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ], y[i] = self.get_tile(img_idx)\n",
    "            return X, y\n",
    "        elif self.mode == 'predict':\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ] = self.get_tile(img_idx)\n",
    "            return X\n",
    "        else:\n",
    "            raise AttributeError('fit mode parameter error')\n",
    "            \n",
    "    def get_tile(self, img_idx):\n",
    "        img_path = f'{self.imgs_path}/{img_idx}.png'\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print('error load image:', img_path)\n",
    "        if self.resize:\n",
    "            img = cv2.resize(img, (int(img.shape[1] / self.resize), int(img.shape[0] / self.resize)))\n",
    "        img = img.astype(np.float32) / 255\n",
    "        if self.mode == 'fit':\n",
    "            msk_path = f'{self.msks_path}/{img_idx}.png'\n",
    "            msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if msk is None:\n",
    "                print('error load mask:', msk_path)\n",
    "            if self.resize:\n",
    "                msk = cv2.resize(msk, (int(msk.shape[1] / self.resize), int(msk.shape[0] / self.resize)))\n",
    "            msk = msk.astype(np.float32)\n",
    "            if self.aug:\n",
    "                augmented = self.aug(image=img, mask=msk)\n",
    "                img = augmented['image']\n",
    "                msk = augmented['mask']\n",
    "            return img, msk\n",
    "        else:\n",
    "            if self.aug:\n",
    "                img = self.aug(image=img)['image']\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_idxs = []\n",
    "for img_idx in tqdm([x.replace('.png', '') \n",
    "                for x in os.listdir(IMGS_PATH) \n",
    "                if '.png' in x]):\n",
    "    msk_path = f'{MSKS_PATH}/{img_idx}.png'\n",
    "    msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if np.sum(msk) == 0:\n",
    "        if PARAMS['downsample']:\n",
    "            if np.random.uniform(0, 1) < PARAMS['downsample']:\n",
    "                imgs_idxs.append(img_idx)\n",
    "        else:\n",
    "            imgs_idxs.append(img_idx)\n",
    "    else:\n",
    "        imgs_idxs.append(img_idx)\n",
    "\n",
    "train_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=imgs_idxs, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=True,           \n",
    "        aug=aug, \n",
    "        resize=None\n",
    ")\n",
    "val_datagen = DataGenKid(\n",
    "    imgs_path=IMGS_PATH, \n",
    "    msks_path=MSKS_PATH, \n",
    "    imgs_idxs=imgs_idxs, \n",
    "    img_size=PARAMS['img_size'], \n",
    "    batch_size=PARAMS['batch_size'], \n",
    "    mode='fit', \n",
    "    shuffle=False,           \n",
    "    aug=None, \n",
    "    resize=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = min(8, PARAMS['batch_size'])\n",
    "Xt, yt = train_datagen.__getitem__(3)\n",
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=2, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    axes[0, j].imshow(Xt[j])\n",
    "    axes[0, j].set_title(j)\n",
    "    axes[0, j].axis('off')\n",
    "    axes[1, j].imshow(yt[j])\n",
    "    axes[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = min(8, PARAMS['batch_size'])\n",
    "Xt, yt = val_datagen.__getitem__(5)\n",
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=2, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    axes[0, j].imshow(Xt[j])\n",
    "    axes[0, j].set_title(j)\n",
    "    axes[0, j].axis('off')\n",
    "    axes[1, j].imshow(yt[j])\n",
    "    axes[1, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1):\n",
    "    return (1 - dice_coef(y_true, y_pred, smooth))\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return PARAMS['bce_weight'] * binary_crossentropy(y_true, y_pred) + \\\n",
    "        (1 - PARAMS['bce_weight']) * dice_loss(y_true, y_pred)\n",
    "\n",
    "def get_model(backbone, input_shape, loss_type='bce_dice', \n",
    "              umodel='unet', classes=1, lr=.001):\n",
    "    with STRATEGY.scope():\n",
    "        if loss_type == 'bce_dice': \n",
    "            loss = bce_dice_loss\n",
    "        elif loss_type == 'bce_jaccard_loss':\n",
    "            loss = bce_jaccard_loss\n",
    "        else:\n",
    "            raise AttributeError('loss mode parameter error')\n",
    "        if umodel == 'unet':\n",
    "            model = Unet(backbone_name=backbone, encoder_weights='imagenet',\n",
    "                         input_shape=input_shape,\n",
    "                         classes=classes, activation='sigmoid')\n",
    "        elif umodel == 'fpn':\n",
    "            model = FPN(backbone_name=backbone, encoder_weights='imagenet',\n",
    "                        input_shape=input_shape,\n",
    "                        classes=classes, activation='sigmoid')\n",
    "        elif umodel == 'link':\n",
    "            model = Linknet(backbone_name=backbone, encoder_weights='imagenet',\n",
    "                            input_shape=input_shape,\n",
    "                            classes=classes, activation='sigmoid')\n",
    "        else:\n",
    "            raise AttributeError('umodel mode parameter error')\n",
    "        model.compile(\n",
    "            optimizer=tfa.optimizers.Lookahead(\n",
    "                tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                sync_period=max(6, int(PARAMS['patience'] / 4))\n",
    "            ),\n",
    "            loss=loss, \n",
    "            metrics=[dice_coef]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=10, epochs=100, warmup=5, plot=False):\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 1e-3\n",
    "    lr_min = lr_start / 100\n",
    "    lr_ramp_ep = warmup\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = .95\n",
    "    \n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "        \n",
    "    if not plot:\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=False)\n",
    "        return lr_callback \n",
    "    else: \n",
    "        return lr_scheduler\n",
    "    \n",
    "if PARAMS['decay']:\n",
    "    lr_scheduler_plot = get_lr_callback(\n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        epochs=PARAMS['epochs'], \n",
    "        plot=True\n",
    "    )\n",
    "    xs = [i for i in range(PARAMS['epochs'])]\n",
    "    y = [lr_scheduler_plot(x) for x in xs]\n",
    "    plt.plot(xs, y)\n",
    "    plt.title(f'lr schedule from {y[0]:.5f} to {max(y):.3f} to {y[-1]:.8f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mparams, n_fold, train_datagen, val_datagen):\n",
    "    model = get_model(\n",
    "        mparams['backbone'], \n",
    "        input_shape=(mparams['img_size'], mparams['img_size'], 3),\n",
    "        loss_type=mparams['loss'],\n",
    "        umodel=mparams['umodel'],\n",
    "        lr=mparams['lr']\n",
    "    )\n",
    "    checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "    earlystopper = EarlyStopping(\n",
    "        monitor='val_dice_coef', \n",
    "        patience=mparams['patience'], \n",
    "        verbose=0,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    lrreducer = ReduceLROnPlateau(\n",
    "        monitor='val_dice_coef', \n",
    "        factor=.1, \n",
    "        patience=int(mparams['patience'] / 2), \n",
    "        verbose=0, \n",
    "        min_lr=1e-7,\n",
    "        mode='max'\n",
    "    )\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        monitor='val_dice_coef', \n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "    callbacks = [earlystopper, checkpointer]\n",
    "    if mparams['decay']:\n",
    "        callbacks.append(get_lr_callback(mparams['batch_size']))\n",
    "        print('lr warmup and decay')\n",
    "    else:\n",
    "        callbacks.append(lrreducer)\n",
    "        print('lr reduce on plateau')\n",
    "    history = model.fit(\n",
    "        train_datagen,\n",
    "        validation_data=val_datagen,\n",
    "        callbacks=callbacks,\n",
    "        epochs=mparams['epochs'],\n",
    "        verbose=1\n",
    "    )\n",
    "    history_file = f'{MDLS_PATH}/history_{n_fold}.json'\n",
    "    dict_to_save = {}\n",
    "    for k, v in history.history.items():\n",
    "        dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "    with open(history_file, 'w') as file:\n",
    "        json.dump(dict_to_save, file)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msks_strat = []\n",
    "for img_idx in tqdm(imgs_idxs):\n",
    "    msk_path = f'{MSKS_PATH}/{img_idx}.png'\n",
    "    msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "    msk_max = PARAMS['img_size'] ** 2\n",
    "    if np.sum(msk) == 0:\n",
    "        msk_cls = 0\n",
    "    elif np.sum(msk) < msk_max * .01:\n",
    "        msk_cls = 1\n",
    "    elif np.sum(msk) < msk_max * .1:\n",
    "        msk_cls = 2\n",
    "    elif np.sum(msk) < msk_max * .2:\n",
    "        msk_cls = 3\n",
    "    elif np.sum(msk) < msk_max * .3:\n",
    "        msk_cls = 4\n",
    "    else:\n",
    "        msk_cls = 5\n",
    "    msks_strat.append(msk_cls)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(msks_strat, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iname in list(set([x[:9] for x in imgs_idxs])):\n",
    "    print('img name:', iname, \n",
    "          '| imgs number:', len([x for x in imgs_idxs if x[:9] == iname]))\n",
    "if PARAMS['split'] == 'kfold':\n",
    "    kfold = KFold(n_splits=PARAMS['folds'],     \n",
    "                  random_state=PARAMS['seed'],\n",
    "                  shuffle=True).split(imgs_idxs)\n",
    "elif PARAMS['split'] == 'group':\n",
    "    grps = [x[:9] for x in imgs_idxs]\n",
    "    kfold = GroupKFold(n_splits=PARAMS['folds']).split(imgs_idxs, imgs_idxs, grps)\n",
    "elif PARAMS['split'] == 'strat':\n",
    "    kfold = StratifiedKFold(n_splits=PARAMS['folds'],     \n",
    "                            random_state=PARAMS['seed'],\n",
    "                            shuffle=True).split(imgs_idxs, msks_strat)\n",
    "else:\n",
    "    raise AttributeError('split mode parameter error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_by_folds = []\n",
    "loss_by_folds = []\n",
    "dice_coef_by_folds = []\n",
    "        \n",
    "for n, (tr, te) in enumerate(kfold):\n",
    "    print('=' * 10, f'FOLD {n}', '=' * 10)\n",
    "    X_tr = [imgs_idxs[i] for i in tr]; X_val = [imgs_idxs[i] for i in te]\n",
    "    M_tr = [msks_strat[i] > 0 for i in tr]; M_val = [msks_strat[i] > 0 for i in te]\n",
    "    print('train:', len(X_tr), '| val:', len(X_val))\n",
    "    print('masks in train:', sum(M_tr) / len(X_tr), \n",
    "          '| masks in val:', sum(M_val) / len(X_val))\n",
    "    print('groups train:', set([x[:9] for x in X_tr]), \n",
    "          '\\ngroups val:', set([x[:9] for x in X_val]))\n",
    "    train_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=X_tr, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=True,           \n",
    "        aug=aug, \n",
    "        resize=None\n",
    "    )\n",
    "    val_datagen = DataGenKid(\n",
    "        imgs_path=IMGS_PATH, \n",
    "        msks_path=MSKS_PATH, \n",
    "        imgs_idxs=X_val, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=False,           \n",
    "        aug=None, \n",
    "        resize=None\n",
    "    )\n",
    "    model, history = train_model(PARAMS, n, train_datagen, val_datagen)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(history.history['dice_coef'], label='dice_coef')\n",
    "    plt.plot(history.history['val_dice_coef'], label='val_dice_coef')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    best_epoch = np.argmax(history.history['val_dice_coef'])\n",
    "    best_loss = history.history['val_loss'][best_epoch]\n",
    "    best_dice_coef = history.history['val_dice_coef'][best_epoch]\n",
    "    print('best epoch:', best_epoch, \n",
    "          '| best loss:', best_loss,\n",
    "          '| best dice coef:', best_dice_coef)\n",
    "    epoch_by_folds.append(best_epoch)\n",
    "    loss_by_folds.append(best_loss)\n",
    "    dice_coef_by_folds.append(best_dice_coef)\n",
    "    del train_datagen, val_datagen, model; gc.collect()\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = PARAMS.copy()\n",
    "result['bavg_epoch'] = np.mean(epoch_by_folds)\n",
    "result['bavg_loss'] = np.mean(loss_by_folds)\n",
    "result['bavg_dice_coef'] = np.mean(dice_coef_by_folds)\n",
    "result['dice_by_folds'] = ' '.join([f'{x:.4f}' for x in dice_coef_by_folds])\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(result, file)\n",
    "if not os.path.exists('results.csv'):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv('results.csv', sep='\\t')\n",
    "else:\n",
    "    df_old = pd.read_csv('results.csv', sep='\\t', index_col=0)\n",
    "    df_save = pd.DataFrame(result, index=[df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv('results.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger = 4\n",
    "test_models = []\n",
    "for n_fold in list(range(PARAMS['folds'])):\n",
    "    checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "    print(checkpoint_path)\n",
    "    model_lrg = get_model(\n",
    "        PARAMS['backbone'], \n",
    "        input_shape=(PARAMS['img_size'] * larger, PARAMS['img_size'] * larger, 3),\n",
    "        loss_type=PARAMS['loss'],\n",
    "        umodel=PARAMS['umodel']\n",
    "    )\n",
    "    model_lrg.load_weights(checkpoint_path) # or .set_weights(model.get_weights()) from smaller model\n",
    "    test_models.append(model_lrg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "resize = PARAMS['resize']\n",
    "shft = .6\n",
    "wnd = PARAMS['img_size'] * larger\n",
    "img = tiff.imread(os.path.join('./data/train', df_masks.index[img_num] + '.tiff'))\n",
    "if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "mask = enc2mask(df_masks.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "print(img.shape, mask.shape)\n",
    "img = cv2.resize(img,\n",
    "                 (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                 interpolation=cv2.INTER_AREA)\n",
    "mask = cv2.resize(mask,\n",
    "                  (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                  interpolation=cv2.INTER_NEAREST)\n",
    "img = img[int(img.shape[0]*shft) : int(img.shape[0]*shft)+wnd, \n",
    "          int(img.shape[1]*shft) : int(img.shape[1]*shft)+wnd, \n",
    "          :]\n",
    "mask = mask[int(mask.shape[0]*shft) : int(mask.shape[0]*shft)+wnd, \n",
    "            int(mask.shape[1]*shft) : int(mask.shape[1]*shft)+wnd]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.imshow(mask, alpha=.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_np(pred, true, k=1):\n",
    "    intersection = np.sum(pred[true==k]) * 2\n",
    "    dice = intersection / (np.sum(pred) + np.sum(true))\n",
    "    return dice\n",
    "\n",
    "def get_dice(mask, mask_lrg, th):\n",
    "    mask_pred = np.squeeze(mask_lrg > th).astype(int)\n",
    "    return dice_np(mask, mask_pred)\n",
    "\n",
    "def get_best_th_dice(mask, mask_lrg, n=100, plot=False):\n",
    "    thresholds = np.linspace(0, 1, n)\n",
    "    dices = [get_dice(mask, mask_lrg, th) for th in thresholds]\n",
    "    n_max = np.argmax(dices)\n",
    "    if plot:\n",
    "        plt.plot(thresholds, dices)\n",
    "        plt.title(f'th: {thresholds[n_max]:.2f} dice: {dices[n_max]:.2f}')\n",
    "        plt.show()\n",
    "    return thresholds[n_max], dices[n_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=len(test_models))\n",
    "for j in range(len(test_models)):\n",
    "    mask_lrg = test_models[j].predict(img[np.newaxis, ] / 255)\n",
    "    axes[j].imshow(np.squeeze(mask_lrg))\n",
    "    axes[j].set_title(f'img {j}: {np.min(mask_lrg):.2f}-{np.max(mask_lrg):.2f}')\n",
    "    axes[j].axis('off')\n",
    "    print(get_best_th_dice(mask, mask_lrg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_th_dice(mask, mask_lrg, n=100, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.hist(mask_lrg.flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.hist(np.where(mask_lrg < 10e-4, np.nan, mask_lrg).flatten(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
